{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tarea RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/edcalderin/DeepLearning_SaturdaysAI/blob/master/Tareas/Tarea_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBpApAN7husX"
      },
      "source": [
        "# Tarea: Recurrent Neural Networks\n",
        "### Grupo: RNN\n",
        "Integrantes:\n",
        "<br>\n",
        "* Erick David Calderin Morales\n",
        "* Sharon Sarai Maygua Mendiola\n",
        "* Rodrigo Antonio Aliaga Vélez\n",
        "\n",
        "<br>\n",
        "\n",
        "Indicaciones:\n",
        "<rb>\n",
        "* Debe realizar la siguiente tarea hasta el miercoles 16 de junio, 23:59 UTC - 4\n",
        "* Debe hacer una copia de este notebook para poder editar el código.\n",
        "* Debe poner el código faltante en las celdas que correspondan.\n",
        "* Una vez finalizado el trabajo debe subir el link de su notebook (con permisos de lector) en la sección de \"Tareas\" del Módulo 3: Recurrent Neural Networks en Eduflow.\n",
        "\n",
        "En la parte práctica de la clase, vimos cómo entrenar una red neuronal recurrente (RNN) para la tarea del análisis del sentimiento. Sin embargo, los resultados en el test set no fueron nada buenos. En este ejercicio, haremos unos cuantos cambios para crear un modelo que nos dé una precisión de más del 80%:\n",
        "\n",
        "\n",
        "\n",
        "*   Cambiar el RNN por un **LSTM bidireccional**\n",
        "*   Utilizar **pre-trained word embeddings** españoles\n",
        "*   Regularización\n",
        "*   Un optimizer distinto\n",
        "*   Procesaremos solo los elementos que no son *padding* (**packed padded sequences**)\n",
        "\n",
        "Para utilizar los embeddings, hay que bajarse el fichero de https://www.kaggle.com/rtatman/pretrained-word-vectors-for-spanish tal y como vimos en la parte práctica. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Psg30h58inUS",
        "outputId": "16777fcd-8097-4a76-f09f-82d519be690b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3E6CU_2kD0G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0faaae54-daad-49c7-b797-c836e375374e"
      },
      "source": [
        "# Bajarse el tokenizer español de SpaCy\n",
        "\n",
        "!python -m spacy download es_core_news_sm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting es_core_news_sm==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-2.2.5/es_core_news_sm-2.2.5.tar.gz (16.2MB)\n",
            "\u001b[K     |████████████████████████████████| 16.2MB 27.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from es_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (57.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->es_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->es_core_news_sm==2.2.5) (2021.5.30)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->es_core_news_sm==2.2.5) (4.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->es_core_news_sm==2.2.5) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->es_core_news_sm==2.2.5) (3.4.1)\n",
            "Building wheels for collected packages: es-core-news-sm\n",
            "  Building wheel for es-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for es-core-news-sm: filename=es_core_news_sm-2.2.5-cp37-none-any.whl size=16172935 sha256=06435598f0b29466804406691b87ca55cc8d440eb4ef1b6fd984cd79e6f652b3\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-j098rg5z/wheels/05/4f/66/9d0c806f86de08e8645d67996798c49e1512f9c3a250d74242\n",
            "Successfully built es-core-news-sm\n",
            "Installing collected packages: es-core-news-sm\n",
            "Successfully installed es-core-news-sm-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('es_core_news_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHC7WiZDkTNV"
      },
      "source": [
        "Recordad que después de bajarse el tokenizer, para que funcione correctamente hay que reanudar el runtime.\n",
        "\n",
        "*Runtime -> Restart runtime*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3hDuvNUkxoB"
      },
      "source": [
        "# Dataset\n",
        "\n",
        "Vamos a inizializar el dataset en torchtext tal y como hicimos en las prácticas. Sin embargo, vamos a añador un parámetro extra en *data.Field* de TEXT llamado *include_lengths=True*. Para poder procesar los elementos que no son padding, necesitamos saber la longitud de cada texto en el dataset.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMzA-C_Akjs1"
      },
      "source": [
        "# Inicializar torchtext dataset\n",
        "\n",
        "import torch\n",
        "from torchtext.legacy import data\n",
        "\n",
        "SEED = 1234\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "TEXT = data.Field(tokenize='spacy', tokenizer_language='es_core_news_sm', include_lengths=True)\n",
        "\n",
        "SENTIMENT = data.LabelField(dtype = torch.float)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsU-gTxImv-F"
      },
      "source": [
        "# Indicar a torchtext qué campos corresponden a los distintos elementos del json\n",
        "\n",
        "fields = {'texto': ('t', TEXT), 'sentimiento': ('s', SENTIMENT)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DbiQqoDa9DqH",
        "outputId": "978c144d-54cb-4f49-b564-462d84818f65"
      },
      "source": [
        "# Echamos un vistazo al corpus\n",
        "\n",
        "import json\n",
        "from pprint import pprint\n",
        "\n",
        "PATH = 'drive/MyDrive/SaturdaysAI/data_sentimiento/'\n",
        "\n",
        "data1 = []\n",
        "with open(PATH + 'train.json') as file:\n",
        "  for line in file:\n",
        "    data1.append(json.loads(line))\n",
        "\n",
        "pprint(data1[:5])\n",
        "print(len(data1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'sentimiento': 'neg',\n",
            "  'texto': 'Llegamos a las 21:30, hicimos los pedidos, solo a las 23 vino la '\n",
            "           'primera porción que pedimos, cuando reclamamos que faltaban '\n",
            "           'porciones nos dijeran que ya se habían agotado (después de ya '\n",
            "           '1:30hr de espera)! A las 24 cuando estábamos por desistir del '\n",
            "           'restante de lo pedido, vino el mozo y garantizo que ya estaban '\n",
            "           'listos! Al fin cuando llegaran no tenían gusto, sin sal, en el '\n",
            "           'bobó de camarao, no había camarao y había pelotas de harina! La '\n",
            "           'verdad muy decepcionante! Fuimos en un grupo de 15 personas, y '\n",
            "           'nadie quedo satisfecho!'},\n",
            " {'sentimiento': 'neg',\n",
            "  'texto': 'la camarera tardó mucho más de lo aceptable en atendernos,jamás '\n",
            "           'trajo el pan,y no tenía ni idea quién había pedido cada cosa.\\r\\n'\n",
            "           'la comida llegó a destiempo (unos comían pizza como entrada y yo '\n",
            "           'una ensalada.. bueno,mi ensalada llegó una vez que habían '\n",
            "           'terminado la pizza).para los principales esperamos como mínimo 40 '\n",
            "           'minutos,  y nuevamente llegaron a destiempo.\\r\\n'\n",
            "           'decir que estábamos con ánimo festivo y les dejamos pasar todo, '\n",
            "           'pero justificar los errores diciendo que \"colapsó la cocina\" no '\n",
            "           'sirve.\\r\\n'\n",
            "           'los tragos espectaculares. tomen el mojito con absolut vanilla,el '\n",
            "           'blody mery,la margarita..todos geniales'},\n",
            " {'sentimiento': 'pos',\n",
            "  'texto': 'Anoche Fui con mi grupo de amigas a comer a vida sana en ramos '\n",
            "           'mejia , fue la primera vez que conociamos el lugar , muy agradable '\n",
            "           ', con musica para relajar , hambiente muy trankilo , nos atendio '\n",
            "           'un camarero muy bien presentable y de aroma rico . . donde nos '\n",
            "           'recomendo unos ñoquis de remolacha , una lazaña sin masa y unos '\n",
            "           'tallarines que fueron fenomenales sinceramente el servicio 10 pts '\n",
            "           'y la comida espetacular , nos gusto mucho y fuimos muy bien '\n",
            "           'atendidas . . es un lugar recomendable para pasar una linda noche '\n",
            "           '!'},\n",
            " {'sentimiento': 'pos',\n",
            "  'texto': 'Recommandé par un ami français de Buenos Aires, nous avons passé '\n",
            "           'une soirée extraordinaire, le lieu est vraiment incroyable, '\n",
            "           \"mention spéciale aux chefs et merci à toute l'équipe UN!k, nous \"\n",
            "           'reviendrons sans aucun doute. Bravo!'},\n",
            " {'sentimiento': 'pos',\n",
            "  'texto': 'Fuimos con un cupón de Pez Urbano, la verdad es que no esperabamos '\n",
            "           'que sea tan rico, nos sorprendió mucho!\\r\\n'\n",
            "           'Lo único que llamamos un montón de veces al número para reservar y '\n",
            "           'nunca nos atendió nadie y sino daba ocupado, así que busqué la '\n",
            "           'página por acá y llamé al otro teléfono y recién ahí pudimos '\n",
            "           'conseguir.\\r\\n'\n",
            "           'Todo el resto buenísimo!'}]\n",
            "800\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCrVaeTSm3AS"
      },
      "source": [
        "Ahora vamos a leer el corpus. Vamos a utilizar los tres ficheros (**train.json**, **valid.json** y **test.json**) que vimos en la parte práctica. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtj19CD3nB6j"
      },
      "source": [
        "# Leer corpus\n",
        "\n",
        "PATH = 'drive/MyDrive/SaturdaysAI/data_sentimiento'\n",
        "\n",
        "train_data, valid_data, test_data = data.TabularDataset.splits(\n",
        "                                                            path = PATH, \n",
        "                                                            train = 'train.json', \n",
        "                                                            validation = 'valid.json', \n",
        "                                                            test = 'test.json',\n",
        "                                                            format = 'json',\n",
        "                                                            fields = fields\n",
        "                                                            )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHdyaDU6nakA"
      },
      "source": [
        "Como vamos a utilizar *pre-trained embeddings*, tenemos que añadirlos cuando construimos el vocabulario. Para inicializar el vocabulario en los pre-trained embeddings a 0, añadimos el parámetro *unk_init*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMhv9213GXOq",
        "outputId": "7b7021c7-6eae-47f4-8cff-787d0999305a"
      },
      "source": [
        "!unzip -u \"drive/MyDrive/SaturdaysAI/words_espanioles.zip\" -d \"drive/MyDrive/SaturdaysAI/\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  drive/MyDrive/SaturdaysAI/words_espanioles.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4pTybBTnsnv"
      },
      "source": [
        "# Leer embeddings, este proceso puede tardar unos segundos\n",
        "\n",
        "import torchtext.vocab as vocab\n",
        "\n",
        "FILE_NAME = 'SBW-vectors-300-min5.txt'\n",
        "PATH = 'drive/MyDrive/SaturdaysAI/'\n",
        "\n",
        "spanish_embeddings = vocab.Vectors(FILE_NAME, cache=PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4LmQnrboR_E"
      },
      "source": [
        "# Constuir vocabulario \n",
        "\n",
        "MAX_VOCAB_SIZE = 4000\n",
        "\n",
        "TEXT.build_vocab(train_data, max_size = MAX_VOCAB_SIZE, \n",
        "                 vectors = spanish_embeddings,\n",
        "                 unk_init = torch.Tensor.normal_)\n",
        "\n",
        "SENTIMENT.build_vocab(train_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o65_gXLvo9tZ"
      },
      "source": [
        "# Preparar train, valid y test iterators para entrenar el modelo\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "     (train_data, valid_data, test_data),\n",
        "     batch_size = BATCH_SIZE,\n",
        "     sort_within_batch = True,\n",
        "     sort_key = lambda x: len(x.t),\n",
        "     device = device\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJFsBheKqIAk",
        "outputId": "a01310ad-e39a-4c49-b86f-da3776218130"
      },
      "source": [
        "# Para ver la dimensión de los pre-trained embeddings\n",
        "\n",
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "\n",
        "print(pretrained_embeddings.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([4002, 300])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Y_yoI2ipNLm"
      },
      "source": [
        "# Modelo\n",
        "\n",
        "Construimos el modelo que entrenaremos y evaluaremos. En esta tarea utilizamos un **LSTM bidireccional**. \n",
        "\n",
        "La *embedding* layer tendrá un parámetro extra, ***padding_idx=pad_idx***, que indica el índice del token *pad* para que el model no lo procese.\n",
        "\n",
        "La *rnn* layer será ahora de tipo **nn.LSTM** con los parámetros siguientes:\n",
        "\n",
        "\n",
        "*   **embedding_dim**: dimensión de los pre-trained embeddings\n",
        "*   **hidden_dim**: dimensión de la hidden layer\n",
        "*   **num_layer**: número de layers\n",
        "*   **bidirectional**: queremos un LSTM bidireccional \n",
        "*   **dropout**: el dropout para regularizar la red neuronal\n",
        "\n",
        "Luego añadimos una **layer linear** y *dropout*. La dimensión de la hidden layer que pasamos por la linear layer es el **doble** porque concatenamos las dos hidden layers con distintas direcciones. \n",
        "\n",
        "También vamos a definir el paso forward. Atención que utilizamos *dropout* y *packed_padded_sequence*. Al final concatenamos las dos hidden layers del Bidirectional LSTM. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKvi23SWpOz2"
      },
      "source": [
        "# Construir modelo\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "class BiLSTM(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout, pad_idx):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=pad_idx)\n",
        "\n",
        "    self.rnn = nn.LSTM(input_size = embedding_dim, \n",
        "                       hidden_size = hidden_dim, \n",
        "                       num_layers = n_layers,\n",
        "                       dropout = dropout, \n",
        "                       bidirectional = bidirectional\n",
        "                       )\n",
        "    self.fc = nn.Linear(2*hidden_dim, output_dim)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  # Forward pass\n",
        "  def forward(self, text, text_lengths):\n",
        "\n",
        "    embedded = self.dropout(self.embedding(text))\n",
        "\n",
        "    packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.to('cpu'))\n",
        "    \n",
        "    packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
        "\n",
        "    output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
        "\n",
        "    hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
        "\n",
        "    return self.fc(hidden)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hVQzeOelptLt"
      },
      "source": [
        "# Definimos parámetros, algunos (como hidden_dim o n_layers) los podéis cambiar y experimentar\n",
        "\n",
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 300\n",
        "HIDDEN_DIM = 450\n",
        "OUTPUT_DIM = 1\n",
        "N_LAYERS = 3\n",
        "BIDIRECTIONAL = True\n",
        "DROPOUT = 0.5\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "\n",
        "# Inicializamos el modelo con todos los parámetros\n",
        "\n",
        "model = BiLSTM(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdtRhTP0qTvE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bef1ae9d-7045-4ee3-a027-b829230b42eb"
      },
      "source": [
        "# Para ver el número de parámetros que entrenaremos en la red neuronal\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'El modelo tiene {count_parameters(model):,} parámetros')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "El modelo tiene 13,643,101 parámetros\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZenuM4Lvfv4"
      },
      "source": [
        "Finalmente, copiamos los pre-trained word embeddings y los metemos en la *embedding layer*. Luego reemplazamos los valores iniciales de la *embedding layer* con los pre-trained embeddings. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VURTTdmIvfSE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c99bb13b-8c2e-41e2-aafb-c003548a5a6c"
      },
      "source": [
        "# Copiamos pre-trained embeddings\n",
        "\n",
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "\n",
        "# Inicializamos embedding layer\n",
        "\n",
        "model.embedding.weight.data.copy_(TEXT.vocab.vectors)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
              "          0.0000e+00,  0.0000e+00],\n",
              "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
              "          0.0000e+00,  0.0000e+00],\n",
              "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
              "          0.0000e+00,  0.0000e+00],\n",
              "        ...,\n",
              "        [-3.9439e-02, -4.2805e-02, -5.0221e-02,  ..., -2.0496e-02,\n",
              "          2.3131e-02, -8.1015e-02],\n",
              "        [ 4.4232e-02, -5.5097e-02,  4.4623e-02,  ..., -7.1947e-02,\n",
              "          7.4214e-02, -2.5300e-02],\n",
              "        [-6.0870e-02, -4.6083e-02,  9.3000e-05,  ..., -8.2717e-02,\n",
              "          1.1736e-01, -4.8698e-02]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNqeZQg7wZP0"
      },
      "source": [
        "# Entrenar \n",
        "\n",
        "Vamos a entrenar el modelo. Empezamos definiendo el *optimizer*. Esta vez vamos a utilizar **Adam**. Utilizamos la misma loss function que vimos en la parte práctica."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8oljxiTwlPw"
      },
      "source": [
        "# Optimizer\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# loss function\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fePKaTFjxE9A"
      },
      "source": [
        "# Modelo y loss function en GPU\n",
        "\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kYTWxvNxFjz"
      },
      "source": [
        "# función para calcular accuracy \n",
        "\n",
        "def binary_accuracy(preds, y):\n",
        "  rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "  correct = rounded_preds == y\n",
        "  acc = correct.sum() / len(correct)\n",
        "  return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bYQQe9SxxuN"
      },
      "source": [
        "Definimos la función para entrenar el modelo. Como que hemos incluido el parámetro *include_lengths=True*, en este caso *batch.t* es una tupla con el primer elemento un tensor de números y el segundo elemento la longitud de cada texto. Con lo cual, antes de pasar *batch.t*, tendremos que separar estos dos elementos:\n",
        "\n",
        "**text, text_lengths = batch.t**\n",
        "\n",
        "Y pasar las dos variables (*text* y *text_lengths*) al modelo. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sheb8LRpxS38"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "\n",
        "  epoch_loss = 0\n",
        "  epoch_acc = 0\n",
        "\n",
        "  model.train()\n",
        "\n",
        "  for batch in iterator:\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    text, text_lengths = batch.t\n",
        "    predictions = model(text, text_lengths).squeeze(1)\n",
        "    loss = criterion(predictions, batch.s)\n",
        "    acc = binary_accuracy(predictions, batch.s)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    epoch_loss += loss.item()\n",
        "    epoch_acc += acc.item()\n",
        "  \n",
        "  return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJVqDLB5zGGI"
      },
      "source": [
        "Para definir la función que evalúa el modelo, recordar que es muy similar a *train*, con alguna diferencia (ver ejercicio práctico). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPCVqfAezQqd"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    model.eval()\n",
        "\n",
        "    for batch in iterator: \n",
        "        text, text_lengths = batch.t   \n",
        "        predictions = model(text, text_lengths).squeeze(1)\n",
        "        loss = criterion(predictions, batch.s)\n",
        "        acc = binary_accuracy(predictions, batch.s)\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuL_zMoezRZH"
      },
      "source": [
        "# Función para saber el tiempo que se tarda para entrenar cada epoch\n",
        "\n",
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtbC-1rNzup2"
      },
      "source": [
        "Vamos a entrenar tal y como lo hicimos en el ejercicio práctico. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyA-VWG6zjRv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de025784-8c15-4ca4-cb9c-80f638b12164"
      },
      "source": [
        "# Entrenamos\n",
        "\n",
        "N_EPOCHS = 10\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'sent-modelo.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.694 | Train Acc: 47.24%\n",
            "\t Val. Loss: 0.687 |  Val. Acc: 64.06%\n",
            "Epoch: 02 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.669 | Train Acc: 59.50%\n",
            "\t Val. Loss: 0.648 |  Val. Acc: 52.86%\n",
            "Epoch: 03 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.355 | Train Acc: 85.94%\n",
            "\t Val. Loss: 0.490 |  Val. Acc: 83.25%\n",
            "Epoch: 04 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.428 | Train Acc: 82.93%\n",
            "\t Val. Loss: 0.431 |  Val. Acc: 82.90%\n",
            "Epoch: 05 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.210 | Train Acc: 91.23%\n",
            "\t Val. Loss: 0.280 |  Val. Acc: 91.32%\n",
            "Epoch: 06 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.121 | Train Acc: 95.43%\n",
            "\t Val. Loss: 0.668 |  Val. Acc: 71.44%\n",
            "Epoch: 07 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.248 | Train Acc: 92.91%\n",
            "\t Val. Loss: 0.837 |  Val. Acc: 71.18%\n",
            "Epoch: 08 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.289 | Train Acc: 91.35%\n",
            "\t Val. Loss: 0.720 |  Val. Acc: 69.01%\n",
            "Epoch: 09 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.315 | Train Acc: 88.70%\n",
            "\t Val. Loss: 0.694 |  Val. Acc: 67.62%\n",
            "Epoch: 10 | Epoch Time: 0m 1s\n",
            "\tTrain Loss: 0.318 | Train Acc: 90.62%\n",
            "\t Val. Loss: 0.752 |  Val. Acc: 70.23%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHo87hbt12Sw"
      },
      "source": [
        "# Test\n",
        "\n",
        "Vamos a ver la precisión del modelo en el test data. Deberías obtener una accuracy alrededor del 80%!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zw0XemwN2BHm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "361bf4b1-eaf4-4cdb-c4a0-206d6cf1383a"
      },
      "source": [
        "# Resultados en el test set\n",
        "\n",
        "MODEL_NAME = 'sent-modelo.pt'\n",
        "\n",
        "model.load_state_dict(torch.load(MODEL_NAME))\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.465 | Test Acc: 84.03%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dE-eQlla2Vib"
      },
      "source": [
        "Si quieres ver cómo funciona el modelo con tus propios comentarios positivos o negativos, podemos crear una función para hacer predicciones.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnbmWk_d2nmO"
      },
      "source": [
        "# Cargar el tokenizer de SpaCy\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load('es_core_news_sm')\n",
        "\n",
        "# Función para predecir sentimiento \n",
        "\n",
        "def predict_sentiment(model, sentence):\n",
        "    # modelo en modo evaluación\n",
        "    model.eval()\n",
        "    # tokenizar texto\n",
        "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
        "    # transformar palabras en sus índices del vocabulario\n",
        "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
        "    length = [len(indexed)]\n",
        "    # convertir lista de índices en tensor\n",
        "    tensor = torch.LongTensor(indexed).to(device)\n",
        "    # añadir una dimensión para batch\n",
        "    tensor = tensor.unsqueeze(1)\n",
        "    # convertir length en un tensor\n",
        "    length_tensor = torch.LongTensor(length)\n",
        "    # predecir, utilizando sigmoid para obtener un número entre 0 y 1\n",
        "    prediction = torch.sigmoid(model(tensor, length_tensor))\n",
        "    if prediction.item() >= 0.5:\n",
        "      return \"negativo\"\n",
        "    else:\n",
        "      return \"positivo\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzPGDs6p36Ze",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9e67a145-6ffd-4b20-f58e-d5db142c5c03"
      },
      "source": [
        "# Podéis probar con el texto que queráis. \n",
        "\n",
        "TEXTO = \"SI ENTENDIMOS ESTA TAREA\"\n",
        "\n",
        "predict_sentiment(model, TEXTO)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'positivo'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BsCBy0RAzQj",
        "outputId": "29795b24-667a-4732-94a3-457973931136"
      },
      "source": [
        "TEXTOS=[\n",
        "        'Mi trabajo es muy dificil',\n",
        "        'Me aumentaron el suelo',\n",
        "        'No me pagaron horas extras',\n",
        "        'Me despidieron',\n",
        "        'El pais lo gobierna un mal presidente',\n",
        "        'Tengo covid',\n",
        "        'Me ascendieron',\n",
        "        'Tengo casa nueva'\n",
        "]\n",
        "for text in TEXTOS:\n",
        "    print(text, '---> Sent:', predict_sentiment(model, text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mi trabajo es muy dificil ---> Sent: positivo\n",
            "Me aumentaron el suelo ---> Sent: positivo\n",
            "No me pagaron horas extras ---> Sent: negativo\n",
            "Me despidieron ---> Sent: positivo\n",
            "El pais lo gobierna un mal presidente ---> Sent: negativo\n",
            "Tengo covid ---> Sent: positivo\n",
            "Me ascendieron ---> Sent: positivo\n",
            "Tengo casa nueva ---> Sent: positivo\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zypzArlyBs4u"
      },
      "source": [
        "Los falsos positivos en las predicciones son evidentes, aún falta por mejorar."
      ]
    }
  ]
}