{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "embeddings.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/edcalderin/DeepLearning_SaturdaysAI/blob/master/3_RecurrentNeuralNets/embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3645nYHP5Vrx"
      },
      "source": [
        "# Word embeddings\n",
        "\n",
        "En la parte teórica hemos visto que para crear *neural networks* para el lenguaje, primero tenemos que transformar el texto en una matriz de vectores. El tipo de vectores que capturan la semántica de las palabras se llaman **word embeddings** y son los que se utilizan en la mayoría de aplicaciones modernas de NLP. Vamos a ver cómo utilizamos embeddings en PyTorch.  \n",
        "\n",
        "En PyTorch se pueden utilizan los *word embeddings* en inglés más típicos (word2vec, GloVe, FastText...) directamente. Nosotros vamos a trabajar con *embeddings* españoles. Recomiendo bajarse estos vectores https://www.kaggle.com/rtatman/pretrained-word-vectors-for-spanish creados por Cristian Cardellino (atención, es un fichero grande de aprox. 3GB)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwgMW1hv5Y_Q",
        "outputId": "b4f3bcfa-025a-4d1b-d8a4-df9233a3630a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdkuI_m-8zZq"
      },
      "source": [
        "# Loading\n",
        "\n",
        "Loading word embeddings en inglés es muy fácil. PyTorch da acceso a los embeddings más típicos a través de la library **torchtext**. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtGSww8n8tnP",
        "outputId": "a8169b66-45c8-4af7-d22c-e84f102fe6cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# load english vectors\n",
        "from torchtext import vocab\n",
        "\n",
        "glove = vocab.GloVe(name='6B', dim=100)\n",
        "\n",
        "print(\"Hay {} palabras en el vocabulario\".format(len(glove.itos)))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:40, 5.38MB/s]                           \n",
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-2-c3745af07af2>\", line 4, in <module>\n",
            "    glove = vocab.GloVe(name='6B', dim=100)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torchtext/vocab.py\", line 487, in __init__\n",
            "    super(GloVe, self).__init__(name, url=url, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torchtext/vocab.py\", line 326, in __init__\n",
            "    self.cache(name, cache, url=url, max_vectors=max_vectors)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torchtext/vocab.py\", line 369, in cache\n",
            "    zf.extractall(cache)\n",
            "  File \"/usr/lib/python3.7/zipfile.py\", line 1636, in extractall\n",
            "    self._extract_member(zipinfo, path, pwd)\n",
            "  File \"/usr/lib/python3.7/zipfile.py\", line 1691, in _extract_member\n",
            "    shutil.copyfileobj(source, target)\n",
            "  File \"/usr/lib/python3.7/shutil.py\", line 79, in copyfileobj\n",
            "    buf = fsrc.read(length)\n",
            "  File \"/usr/lib/python3.7/zipfile.py\", line 907, in read\n",
            "    def read(self, n=-1):\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 1823, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 1132, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.7/inspect.py\", line 1464, in getframeinfo\n",
            "    lines, lnum = findsource(frame)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 182, in findsource\n",
            "    lines = linecache.getlines(file, globals_dict)\n",
            "  File \"/usr/lib/python3.7/linecache.py\", line 47, in getlines\n",
            "    return updatecache(filename, module_globals)\n",
            "  File \"/usr/lib/python3.7/linecache.py\", line 136, in updatecache\n",
            "    with tokenize.open(fullname) as fp:\n",
            "  File \"/usr/lib/python3.7/tokenize.py\", line 449, in open\n",
            "    encoding, lines = detect_encoding(buffer.readline)\n",
            "  File \"/usr/lib/python3.7/tokenize.py\", line 418, in detect_encoding\n",
            "    first = read_or_stop()\n",
            "  File \"/usr/lib/python3.7/tokenize.py\", line 376, in read_or_stop\n",
            "    return readline()\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NF36466B-O_Q"
      },
      "source": [
        "Para acceder a los vectores españoles, tenemos que indicar dónde está el fichero."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BetUVtR-90t9"
      },
      "source": [
        "# load spanish vectors\n",
        "\n",
        "es_vectors = vocab.Vectors('SBW-vectors-300-min5.txt', cache='drive/MyDrive/SaturdaysAI')\n",
        "\n",
        "print(\"Hay {} palabras en el vocabulario\".format(len(es_vectors.itos)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxjrZSuA-yCY",
        "outputId": "1fee690f-adf2-417d-b1a0-9b1bbd64f0c1"
      },
      "source": [
        "# ver dimensiones\n",
        "\n",
        "es_vectors.vectors.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1000653, 300])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rgFsGOe_6Ia"
      },
      "source": [
        "# Examinar los embeddings\n",
        "\n",
        "Podemos examinar los embeddings individualmente y ver qué vector está asociado con qué palabra del vocabulario. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4fLwHNn_lT6",
        "outputId": "820bebbd-8485-415d-8c16-b5ccd05bf1f7"
      },
      "source": [
        "# encontrar el índice de una palabra\n",
        "\n",
        "es_vectors.stoi['perro']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5880"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fd7A80IXArk4"
      },
      "source": [
        "# examinar vector\n",
        "\n",
        "es_vectors.vectors[5880]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ED4ee4s4A_12"
      },
      "source": [
        "# out-of-vocabulary words\n",
        "\n",
        "es_vectors.stoi['prro']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-6uxSdrBNxt"
      },
      "source": [
        "# funcion que extrae el word embedding para una palabra\n",
        "\n",
        "def get_vector(embeddings, word):\n",
        "    assert word in embeddings.stoi, f'*{word}* no se encuentra en el vocabulario!'\n",
        "    return embeddings.vectors[embeddings.stoi[word]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_HMlMDDBoHL",
        "outputId": "d05ce27b-6e17-43fb-87d8-2d95f0551a9f"
      },
      "source": [
        "# examinar vector\n",
        "\n",
        "get_vector(es_vectors, 'perro').shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([300])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AfuS_JEDWxa"
      },
      "source": [
        "# Contextos similares\n",
        "\n",
        "Para encontrar palabras similares a una palabra en concreto, primero tenemos que encontrar el vector de esta palabra y luego calcular la distancia entre este vector y los vectores del resto de las palabras. Luego los ordenamos de más cerca a más lejano. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKFWoURLBwZI"
      },
      "source": [
        "# Función para encontrar palabras más similares\n",
        "\n",
        "import torch\n",
        "\n",
        "def closest_words(embeddings, vector, n = 10):\n",
        "    \n",
        "    distances = [(word, torch.dist(vector, get_vector(embeddings, word)).item())\n",
        "                 for word in embeddings.itos]\n",
        "    \n",
        "    return sorted(distances, key = lambda w: w[1])[:n]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LuSNI-GpESvC",
        "outputId": "113d2eda-3e0f-406b-d0fa-94901b147413"
      },
      "source": [
        "# Buscar vectores más cercanos\n",
        "\n",
        "word_vector = get_vector(es_vectors, 'perro')\n",
        "\n",
        "closest_words(es_vectors, word_vector)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('perro', 0.0),\n",
              " ('perros', 0.7023846507072449),\n",
              " ('cachorro', 0.7027554512023926),\n",
              " ('gato', 0.7147189378738403),\n",
              " ('schnauzer', 0.7251959443092346),\n",
              " ('mastín', 0.7283346056938171),\n",
              " ('caniche', 0.7307871580123901),\n",
              " ('teckel', 0.7311853170394897),\n",
              " ('pinscher', 0.7341269850730896),\n",
              " ('collie', 0.7447739243507385)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCuRCTKcGP18"
      },
      "source": [
        "# Analogía\n",
        "\n",
        "Otra propiedad de los *word embeddings* es que podemos hacer operaciones como si fueran vectores normales, con resultados interesantes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zv6gP3RzGTel"
      },
      "source": [
        "def analogy(embeddings, word1, word2, word3, n=5):\n",
        "    \n",
        "    #obtener vectores para cada palaba\n",
        "    word1_vector = get_vector(embeddings, word1)\n",
        "    word2_vector = get_vector(embeddings, word2)\n",
        "    word3_vector = get_vector(embeddings, word3)\n",
        "    \n",
        "    #calcularel vector análogo\n",
        "    analogy_vector = word2_vector - word1_vector + word3_vector\n",
        "    \n",
        "    #encontrar palabras más cercanas\n",
        "    candidate_words = closest_words(embeddings, analogy_vector, n+3)\n",
        "    \n",
        "    #filtrar palabras que ya se encuentran en la analogía\n",
        "    candidate_words = [(word, dist) for (word, dist) in candidate_words \n",
        "                       if word not in [word1, word2, word3]][:n]\n",
        "    \n",
        "    print(f'{word1} es a {word2} como {word3} es a...')\n",
        "    \n",
        "    return candidate_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEAamAPHHbmW"
      },
      "source": [
        "def print_tuples(tuples):\n",
        "    for w, d in tuples:\n",
        "        print(f'({d:02.04f}) {w}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1X0yOMOHe-5"
      },
      "source": [
        "# buscar analogía\n",
        "\n",
        "print_tuples(analogy(es_vectors, \"rey\", \"hombre\", \"reina\"))\n",
        "print_tuples(analogy(es_vectors, 'perro', 'cachorro', 'gato'))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}