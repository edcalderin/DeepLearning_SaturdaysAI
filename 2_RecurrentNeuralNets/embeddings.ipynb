{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "embeddings.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/edcalderin/DeepLearning_SaturdaysAI/blob/master/2_RecurrentNeuralNets/embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3645nYHP5Vrx"
      },
      "source": [
        "# Word embeddings\n",
        "\n",
        "En la parte teórica hemos visto que para crear *neural networks* para el lenguaje, primero tenemos que transformar el texto en una matriz de vectores. El tipo de vectores que capturan la semántica de las palabras se llaman **word embeddings** y son los que se utilizan en la mayoría de aplicaciones modernas de NLP. Vamos a ver cómo utilizamos embeddings en PyTorch.  \n",
        "\n",
        "En PyTorch se pueden utilizan los *word embeddings* en inglés más típicos (word2vec, GloVe, FastText...) directamente. Nosotros vamos a trabajar con *embeddings* españoles. Recomiendo bajarse estos vectores https://www.kaggle.com/rtatman/pretrained-word-vectors-for-spanish creados por Cristian Cardellino (atención, es un fichero grande de aprox. 3GB)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwgMW1hv5Y_Q",
        "outputId": "bb06abd5-a2f6-4135-b55c-f02eb88ebd0c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdkuI_m-8zZq"
      },
      "source": [
        "# Loading\n",
        "\n",
        "Loading word embeddings en inglés es muy fácil. PyTorch da acceso a los embeddings más típicos a través de la library **torchtext**. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtGSww8n8tnP"
      },
      "source": [
        "# load english vectors\n",
        "\n",
        "import torchtext.vocab\n",
        "\n",
        "glove = torchtext.vocab.GloVe(name='6B', dim=100)\n",
        "\n",
        "print(\"Hay {} palabras en el vocabulario\".format(len(glove.itos)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NF36466B-O_Q"
      },
      "source": [
        "Para acceder a los vectores españoles, tenemos que indicar dónde está el fichero."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BetUVtR-90t9",
        "outputId": "c9c3b36c-087d-4a0b-fea0-520ce86331ca"
      },
      "source": [
        "# load spanish vectors\n",
        "\n",
        "es_vectors = torchtext.vocab.Vectors('SBW-vectors-300-min5.txt', cache='drive/MyDrive/Saturdays.AI')\n",
        "\n",
        "print(\"Hay {} palabras en el vocabulario\".format(len(es_vectors.itos)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hay 1000653 palabras en el vocabulario\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxjrZSuA-yCY",
        "outputId": "1fee690f-adf2-417d-b1a0-9b1bbd64f0c1"
      },
      "source": [
        "# ver dimensiones\n",
        "\n",
        "es_vectors.vectors.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1000653, 300])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rgFsGOe_6Ia"
      },
      "source": [
        "# Examinar los embeddings\n",
        "\n",
        "Podemos examinar los embeddings individualmente y ver qué vector está asociado con qué palabra del vocabulario. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4fLwHNn_lT6",
        "outputId": "820bebbd-8485-415d-8c16-b5ccd05bf1f7"
      },
      "source": [
        "# encontrar el índice de una palabra\n",
        "\n",
        "es_vectors.stoi['perro']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5880"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fd7A80IXArk4"
      },
      "source": [
        "# examinar vector\n",
        "\n",
        "es_vectors.vectors[5880]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ED4ee4s4A_12"
      },
      "source": [
        "# out-of-vocabulary words\n",
        "\n",
        "es_vectors.stoi['prro']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-6uxSdrBNxt"
      },
      "source": [
        "# funcion que extrae el word embedding para una palabra\n",
        "\n",
        "def get_vector(embeddings, word):\n",
        "    assert word in embeddings.stoi, f'*{word}* no se encuentra en el vocabulario!'\n",
        "    return embeddings.vectors[embeddings.stoi[word]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_HMlMDDBoHL",
        "outputId": "d05ce27b-6e17-43fb-87d8-2d95f0551a9f"
      },
      "source": [
        "# examinar vector\n",
        "\n",
        "get_vector(es_vectors, 'perro').shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([300])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AfuS_JEDWxa"
      },
      "source": [
        "# Contextos similares\n",
        "\n",
        "Para encontrar palabras similares a una palabra en concreto, primero tenemos que encontrar el vector de esta palabra y luego calcular la distancia entre este vector y los vectores del resto de las palabras. Luego los ordenamos de más cerca a más lejano. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKFWoURLBwZI"
      },
      "source": [
        "# Función para encontrar palabras más similares\n",
        "\n",
        "import torch\n",
        "\n",
        "def closest_words(embeddings, vector, n = 10):\n",
        "    \n",
        "    distances = [(word, torch.dist(vector, get_vector(embeddings, word)).item())\n",
        "                 for word in embeddings.itos]\n",
        "    \n",
        "    return sorted(distances, key = lambda w: w[1])[:n]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LuSNI-GpESvC",
        "outputId": "113d2eda-3e0f-406b-d0fa-94901b147413"
      },
      "source": [
        "# Buscar vectores más cercanos\n",
        "\n",
        "word_vector = get_vector(es_vectors, 'perro')\n",
        "\n",
        "closest_words(es_vectors, word_vector)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('perro', 0.0),\n",
              " ('perros', 0.7023846507072449),\n",
              " ('cachorro', 0.7027554512023926),\n",
              " ('gato', 0.7147189378738403),\n",
              " ('schnauzer', 0.7251959443092346),\n",
              " ('mastín', 0.7283346056938171),\n",
              " ('caniche', 0.7307871580123901),\n",
              " ('teckel', 0.7311853170394897),\n",
              " ('pinscher', 0.7341269850730896),\n",
              " ('collie', 0.7447739243507385)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCuRCTKcGP18"
      },
      "source": [
        "# Analogía\n",
        "\n",
        "Otra propiedad de los *word embeddings* es que podemos hacer operaciones como si fueran vectores normales, con resultados interesantes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zv6gP3RzGTel"
      },
      "source": [
        "def analogy(embeddings, word1, word2, word3, n=5):\n",
        "    \n",
        "    #obtener vectores para cada palaba\n",
        "    word1_vector = get_vector(embeddings, word1)\n",
        "    word2_vector = get_vector(embeddings, word2)\n",
        "    word3_vector = get_vector(embeddings, word3)\n",
        "    \n",
        "    #calcularel vector análogo\n",
        "    analogy_vector = word2_vector - word1_vector + word3_vector\n",
        "    \n",
        "    #encontrar palabras más cercanas\n",
        "    candidate_words = closest_words(embeddings, analogy_vector, n+3)\n",
        "    \n",
        "    #filtrar palabras que ya se encuentran en la analogía\n",
        "    candidate_words = [(word, dist) for (word, dist) in candidate_words \n",
        "                       if word not in [word1, word2, word3]][:n]\n",
        "    \n",
        "    print(f'{word1} es a {word2} como {word3} es a...')\n",
        "    \n",
        "    return candidate_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEAamAPHHbmW"
      },
      "source": [
        "def print_tuples(tuples):\n",
        "    for w, d in tuples:\n",
        "        print(f'({d:02.04f}) {w}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1X0yOMOHe-5"
      },
      "source": [
        "# buscar analogía\n",
        "\n",
        "print_tuples(analogy(es_vectors, \"rey\", \"hombre\", \"reina\"))\n",
        "print_tuples(analogy(es_vectors, 'perro', 'cachorro', 'gato'))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}